{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import keras_tuner as kt\n",
    "import random\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "data= pd.read_excel('C://Users//DanukaDilshanRathnay//Desktop//AI-Driven-Job-Role-Fit-Prediction//code//Dataset//Similarity_with_EMP_data.xlsx')\n",
    "# data=pd.read_excel('C://Users//DanukaDilshanRathnay//Desktop//AI-Driven-Job-Role-Fit-Prediction//Merge_data_new8.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data[data['Department']=='Data and AI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EmployeeCode</th>\n",
       "      <th>Technical Score_JD</th>\n",
       "      <th>Programming Score_JD</th>\n",
       "      <th>Soft Score_with_JD</th>\n",
       "      <th>Education_match_Score_with_JD</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Department</th>\n",
       "      <th>JobCategory</th>\n",
       "      <th>ProficiencyLevel</th>\n",
       "      <th>...</th>\n",
       "      <th>Total Experience in Years</th>\n",
       "      <th>Number of Goal Assigned</th>\n",
       "      <th>Number of Goals Achieved</th>\n",
       "      <th>Final Score</th>\n",
       "      <th>Goals Score</th>\n",
       "      <th>Competency Score</th>\n",
       "      <th>Cultural Value Score</th>\n",
       "      <th>Additional Accomplishment Score</th>\n",
       "      <th>Potential Assessment Score</th>\n",
       "      <th>Trait Assessment Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EMP9004</td>\n",
       "      <td>0.257062</td>\n",
       "      <td>0.667620</td>\n",
       "      <td>0.468664</td>\n",
       "      <td>0.573545</td>\n",
       "      <td>Male</td>\n",
       "      <td>41</td>\n",
       "      <td>Data and AI</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Senior</td>\n",
       "      <td>...</td>\n",
       "      <td>19</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>34.180251</td>\n",
       "      <td>76.488103</td>\n",
       "      <td>7.08</td>\n",
       "      <td>51.806653</td>\n",
       "      <td>55.141563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EMP9005</td>\n",
       "      <td>0.171806</td>\n",
       "      <td>0.543745</td>\n",
       "      <td>0.468005</td>\n",
       "      <td>0.648808</td>\n",
       "      <td>Female</td>\n",
       "      <td>33</td>\n",
       "      <td>Data and AI</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Mid-Level</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>30.775600</td>\n",
       "      <td>63.895969</td>\n",
       "      <td>1.63</td>\n",
       "      <td>40.425636</td>\n",
       "      <td>54.988743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EMP9009</td>\n",
       "      <td>0.237569</td>\n",
       "      <td>0.667620</td>\n",
       "      <td>0.509374</td>\n",
       "      <td>0.675198</td>\n",
       "      <td>Male</td>\n",
       "      <td>35</td>\n",
       "      <td>Data and AI</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Mid-Level</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>42.857143</td>\n",
       "      <td>20.708582</td>\n",
       "      <td>57.979997</td>\n",
       "      <td>8.01</td>\n",
       "      <td>30.443135</td>\n",
       "      <td>36.977535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EMP9015</td>\n",
       "      <td>0.253079</td>\n",
       "      <td>0.646303</td>\n",
       "      <td>0.453373</td>\n",
       "      <td>0.634185</td>\n",
       "      <td>Male</td>\n",
       "      <td>42</td>\n",
       "      <td>Data and AI</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Senior</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>18.998407</td>\n",
       "      <td>66.495545</td>\n",
       "      <td>5.71</td>\n",
       "      <td>43.893732</td>\n",
       "      <td>45.595596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EMP9021</td>\n",
       "      <td>0.163569</td>\n",
       "      <td>0.646303</td>\n",
       "      <td>0.418016</td>\n",
       "      <td>0.721195</td>\n",
       "      <td>Male</td>\n",
       "      <td>59</td>\n",
       "      <td>Data and AI</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Expert</td>\n",
       "      <td>...</td>\n",
       "      <td>37</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>56.717347</td>\n",
       "      <td>73.333333</td>\n",
       "      <td>32.126666</td>\n",
       "      <td>78.174790</td>\n",
       "      <td>2.63</td>\n",
       "      <td>61.771810</td>\n",
       "      <td>50.283365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  EmployeeCode  Technical Score_JD  Programming Score_JD  Soft Score_with_JD  \\\n",
       "0      EMP9004            0.257062              0.667620            0.468664   \n",
       "1      EMP9005            0.171806              0.543745            0.468005   \n",
       "2      EMP9009            0.237569              0.667620            0.509374   \n",
       "3      EMP9015            0.253079              0.646303            0.453373   \n",
       "4      EMP9021            0.163569              0.646303            0.418016   \n",
       "\n",
       "   Education_match_Score_with_JD  Gender  Age   Department     JobCategory  \\\n",
       "0                       0.573545    Male   41  Data and AI  Data Scientist   \n",
       "1                       0.648808  Female   33  Data and AI   Data Engineer   \n",
       "2                       0.675198    Male   35  Data and AI  Data Scientist   \n",
       "3                       0.634185    Male   42  Data and AI  Data Scientist   \n",
       "4                       0.721195    Male   59  Data and AI    Data Analyst   \n",
       "\n",
       "  ProficiencyLevel  ... Total Experience in Years  Number of Goal Assigned  \\\n",
       "0           Senior  ...                        19                       10   \n",
       "1        Mid-Level  ...                        11                       10   \n",
       "2        Mid-Level  ...                        13                        7   \n",
       "3           Senior  ...                        20                       10   \n",
       "4           Expert  ...                        37                       15   \n",
       "\n",
       "  Number of Goals Achieved Final Score Goals Score  Competency Score  \\\n",
       "0                        7   35.000000   70.000000         34.180251   \n",
       "1                        5   35.000000   50.000000         30.775600   \n",
       "2                        3   35.000000   42.857143         20.708582   \n",
       "3                        5   35.000000   50.000000         18.998407   \n",
       "4                       11   56.717347   73.333333         32.126666   \n",
       "\n",
       "   Cultural Value Score  Additional Accomplishment Score  \\\n",
       "0             76.488103                             7.08   \n",
       "1             63.895969                             1.63   \n",
       "2             57.979997                             8.01   \n",
       "3             66.495545                             5.71   \n",
       "4             78.174790                             2.63   \n",
       "\n",
       "   Potential Assessment Score  Trait Assessment Score  \n",
       "0                   51.806653               55.141563  \n",
       "1                   40.425636               54.988743  \n",
       "2                   30.443135               36.977535  \n",
       "3                   43.893732               45.595596  \n",
       "4                   61.771810               50.283365  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_ids =data[\"EmployeeCode\"]\n",
    "\n",
    "data= data.drop(columns=[\"EmployeeCode\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "un_col=['Gender','Department', 'JobCategory', 'ProficiencyLevel',\n",
    "       'Education Qualifications', 'Professional Qualifications','Final Score','Age','List of Software Skills','Number of Goal Assigned', 'Number of Goals Achieved','Projects Completed',\"Total Experience in Years\",\"Absentism Rate\"]\n",
    "df=data.drop(columns=un_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Technical Score_JD</th>\n",
       "      <th>Programming Score_JD</th>\n",
       "      <th>Soft Score_with_JD</th>\n",
       "      <th>Education_match_Score_with_JD</th>\n",
       "      <th>Years of Experience in this Company</th>\n",
       "      <th>KPI</th>\n",
       "      <th>Employee Satisfaction Score</th>\n",
       "      <th>Experience in Years Previous Positions</th>\n",
       "      <th>Goals Score</th>\n",
       "      <th>Competency Score</th>\n",
       "      <th>Cultural Value Score</th>\n",
       "      <th>Additional Accomplishment Score</th>\n",
       "      <th>Potential Assessment Score</th>\n",
       "      <th>Trait Assessment Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.257062</td>\n",
       "      <td>0.667620</td>\n",
       "      <td>0.468664</td>\n",
       "      <td>0.573545</td>\n",
       "      <td>5</td>\n",
       "      <td>38.865258</td>\n",
       "      <td>82.68</td>\n",
       "      <td>14</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>34.180251</td>\n",
       "      <td>76.488103</td>\n",
       "      <td>7.08</td>\n",
       "      <td>51.806653</td>\n",
       "      <td>55.141563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.171806</td>\n",
       "      <td>0.543745</td>\n",
       "      <td>0.468005</td>\n",
       "      <td>0.648808</td>\n",
       "      <td>6</td>\n",
       "      <td>37.089484</td>\n",
       "      <td>68.07</td>\n",
       "      <td>5</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>30.775600</td>\n",
       "      <td>63.895969</td>\n",
       "      <td>1.63</td>\n",
       "      <td>40.425636</td>\n",
       "      <td>54.988743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.237569</td>\n",
       "      <td>0.667620</td>\n",
       "      <td>0.509374</td>\n",
       "      <td>0.675198</td>\n",
       "      <td>2</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>82.60</td>\n",
       "      <td>11</td>\n",
       "      <td>42.857143</td>\n",
       "      <td>20.708582</td>\n",
       "      <td>57.979997</td>\n",
       "      <td>8.01</td>\n",
       "      <td>30.443135</td>\n",
       "      <td>36.977535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.253079</td>\n",
       "      <td>0.646303</td>\n",
       "      <td>0.453373</td>\n",
       "      <td>0.634185</td>\n",
       "      <td>2</td>\n",
       "      <td>31.188340</td>\n",
       "      <td>72.13</td>\n",
       "      <td>18</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>18.998407</td>\n",
       "      <td>66.495545</td>\n",
       "      <td>5.71</td>\n",
       "      <td>43.893732</td>\n",
       "      <td>45.595596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.163569</td>\n",
       "      <td>0.646303</td>\n",
       "      <td>0.418016</td>\n",
       "      <td>0.721195</td>\n",
       "      <td>14</td>\n",
       "      <td>52.928402</td>\n",
       "      <td>96.43</td>\n",
       "      <td>23</td>\n",
       "      <td>73.333333</td>\n",
       "      <td>32.126666</td>\n",
       "      <td>78.174790</td>\n",
       "      <td>2.63</td>\n",
       "      <td>61.771810</td>\n",
       "      <td>50.283365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Technical Score_JD  Programming Score_JD  Soft Score_with_JD  \\\n",
       "0            0.257062              0.667620            0.468664   \n",
       "1            0.171806              0.543745            0.468005   \n",
       "2            0.237569              0.667620            0.509374   \n",
       "3            0.253079              0.646303            0.453373   \n",
       "4            0.163569              0.646303            0.418016   \n",
       "\n",
       "   Education_match_Score_with_JD  Years of Experience in this Company  \\\n",
       "0                       0.573545                                    5   \n",
       "1                       0.648808                                    6   \n",
       "2                       0.675198                                    2   \n",
       "3                       0.634185                                    2   \n",
       "4                       0.721195                                   14   \n",
       "\n",
       "         KPI  Employee Satisfaction Score  \\\n",
       "0  38.865258                        82.68   \n",
       "1  37.089484                        68.07   \n",
       "2  30.000000                        82.60   \n",
       "3  31.188340                        72.13   \n",
       "4  52.928402                        96.43   \n",
       "\n",
       "   Experience in Years Previous Positions  Goals Score  Competency Score  \\\n",
       "0                                      14    70.000000         34.180251   \n",
       "1                                       5    50.000000         30.775600   \n",
       "2                                      11    42.857143         20.708582   \n",
       "3                                      18    50.000000         18.998407   \n",
       "4                                      23    73.333333         32.126666   \n",
       "\n",
       "   Cultural Value Score  Additional Accomplishment Score  \\\n",
       "0             76.488103                             7.08   \n",
       "1             63.895969                             1.63   \n",
       "2             57.979997                             8.01   \n",
       "3             66.495545                             5.71   \n",
       "4             78.174790                             2.63   \n",
       "\n",
       "   Potential Assessment Score  Trait Assessment Score  \n",
       "0                   51.806653               55.141563  \n",
       "1                   40.425636               54.988743  \n",
       "2                   30.443135               36.977535  \n",
       "3                   43.893732               45.595596  \n",
       "4                   61.771810               50.283365  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize numerical data\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train/test sets\n",
    "X_train, X_test = train_test_split(scaled_data, test_size=0.3, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define Autoencoder Model\n",
    "# input_dim = X_train.shape[1]\n",
    "# encoding_dim = 9  # Latent space\n",
    "\n",
    "# input_layer = Input(shape=(input_dim,))\n",
    "# encoded = Dense(12, activation='relu')(input_layer)\n",
    "# encoded = Dense(encoding_dim, activation='relu')(encoded)\n",
    "# decoded = Dense(12, activation='relu')(encoded)\n",
    "# decoded = Dense(input_dim, activation='sigmoid')(decoded)\n",
    "\n",
    "# autoencoder = Model(input_layer, decoded)\n",
    "# autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# # Train Autoencoder\n",
    "# autoencoder.fit(X_train, X_train, epochs=50, batch_size=16, shuffle=True, validation_data=(X_test, X_test))\n",
    "\n",
    "# # Extract Encoder Model\n",
    "# encoder = Model(input_layer, encoded)\n",
    "\n",
    "# # Generate latent space representation (Employee Scores)\n",
    "# employee_scores = encoder.predict(scaled_data)\n",
    "\n",
    "# # Add latent score to dataset\n",
    "# for i in range(encoding_dim):\n",
    "#     data[f'LatentFeature_{i+1}'] = employee_scores[:, i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from autoencoder_tuning\\employee_autoencoder\\tuner0.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.regularizers import l2, l1\n",
    "import keras_tuner as kt\n",
    "\n",
    "def build_autoencoder(hp):\n",
    "    input_dim = X_train.shape[1]\n",
    "    encoding_dim = hp.Int(\"encoding_dim\", min_value=2, max_value=max(4, input_dim // 2), step=1)  # Reduced max_value\n",
    "\n",
    "    # Regularization strength hyperparameters (more conservative range)\n",
    "    l2_reg = hp.Float(\"l2_reg\", min_value=1e-6, max_value=1e-4, sampling=\"log\")\n",
    "    dropout_rate = hp.Float(\"dropout_rate\", min_value=0.05, max_value=0.2, step=0.05)\n",
    "    l1_reg = hp.Float(\"l1_reg\", min_value=1e-6, max_value=1e-4, sampling=\"log\")  # Sparsity regularization\n",
    "\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "\n",
    "    # First hidden layer with L2 regularization and dropout\n",
    "    # Fewer units in the first layer\n",
    "    encoded = Dense(hp.Int(\"units1\", min_value=4, max_value=32, step=4),\n",
    "                    activation=hp.Choice(\"activation1\", [\"relu\", \"tanh\"]),\n",
    "                    kernel_regularizer=l2(l2_reg))(input_layer)\n",
    "    encoded = Dropout(dropout_rate)(encoded)\n",
    "\n",
    "    # Latent space with L1 sparsity constraint\n",
    "    encoded = Dense(encoding_dim, activation=\"relu\", activity_regularizer=l1(l1_reg))(encoded)\n",
    "\n",
    "    # Decoder with L2 regularization and dropout\n",
    "    # Simpler decoder\n",
    "    decoded = Dense(hp.Int(\"units2\", min_value=4, max_value=32, step=4),\n",
    "                    activation=hp.Choice(\"activation2\", [\"relu\", \"tanh\"]),\n",
    "                    kernel_regularizer=l2(l2_reg))(encoded)\n",
    "    decoded = Dropout(dropout_rate)(decoded)\n",
    "    decoded = Dense(input_dim, activation=\"sigmoid\")(decoded)\n",
    "\n",
    "    autoencoder = Model(input_layer, decoded)\n",
    "    autoencoder.compile(optimizer=Adam(learning_rate=hp.Choice(\"learning_rate\", [0.001, 0.0005, 0.0001])),\n",
    "                        loss=\"mse\")\n",
    "\n",
    "    return autoencoder\n",
    "\n",
    "# Hyperparameter tuning with Keras Tuner\n",
    "tuner = kt.RandomSearch(\n",
    "    build_autoencoder,\n",
    "    objective=\"val_loss\",\n",
    "    max_trials=5,  # Fewer trials to prevent excessive fitting to the small dataset\n",
    "    executions_per_trial=1,  # Reduce the number of executions to minimize variance effects\n",
    "    directory=\"autoencoder_tuning\",\n",
    "    project_name=\"employee_autoencoder\"\n",
    ")\n",
    "\n",
    "# Search for the best hyperparameters\n",
    "tuner.search(X_train, X_train, epochs=20, batch_size=8, validation_data=(X_test, X_test))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Encoding Dim: 4\n",
      "Best Layer 1 Units: 8, Activation: relu\n",
      "Best Layer 2 Units: 28, Activation: tanh\n",
      "Best Learning Rate: 0.001\n",
      "Epoch 1/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 0.0786 - val_loss: 0.0797\n",
      "Epoch 2/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0745 - val_loss: 0.0771\n",
      "Epoch 3/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0713 - val_loss: 0.0747\n",
      "Epoch 4/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0685 - val_loss: 0.0721\n",
      "Epoch 5/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0655 - val_loss: 0.0693\n",
      "Epoch 6/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0624 - val_loss: 0.0664\n",
      "Epoch 7/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0595 - val_loss: 0.0637\n",
      "Epoch 8/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0571 - val_loss: 0.0613\n",
      "Epoch 9/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0542 - val_loss: 0.0592\n",
      "Epoch 10/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0525 - val_loss: 0.0574\n",
      "Epoch 11/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0508 - val_loss: 0.0558\n",
      "Epoch 12/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0497 - val_loss: 0.0544\n",
      "Epoch 13/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0492 - val_loss: 0.0534\n",
      "Epoch 14/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0484 - val_loss: 0.0524\n",
      "Epoch 15/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0468 - val_loss: 0.0513\n",
      "Epoch 16/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0457 - val_loss: 0.0505\n",
      "Epoch 17/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0456 - val_loss: 0.0497\n",
      "Epoch 18/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0455 - val_loss: 0.0491\n",
      "Epoch 19/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0441 - val_loss: 0.0484\n",
      "Epoch 20/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0438 - val_loss: 0.0479\n",
      "Epoch 21/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0437 - val_loss: 0.0475\n",
      "Epoch 22/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0431 - val_loss: 0.0472\n",
      "Epoch 23/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0436 - val_loss: 0.0468\n",
      "Epoch 24/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0424 - val_loss: 0.0467\n",
      "Epoch 25/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0427 - val_loss: 0.0463\n",
      "Epoch 26/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0426 - val_loss: 0.0460\n",
      "Epoch 27/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0422 - val_loss: 0.0459\n",
      "Epoch 28/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0421 - val_loss: 0.0456\n",
      "Epoch 29/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0417 - val_loss: 0.0453\n",
      "Epoch 30/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0409 - val_loss: 0.0450\n",
      "Epoch 31/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0411 - val_loss: 0.0448\n",
      "Epoch 32/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0406 - val_loss: 0.0447\n",
      "Epoch 33/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0402 - val_loss: 0.0443\n",
      "Epoch 34/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0402 - val_loss: 0.0441\n",
      "Epoch 35/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0399 - val_loss: 0.0437\n",
      "Epoch 36/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0396 - val_loss: 0.0436\n",
      "Epoch 37/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0394 - val_loss: 0.0433\n",
      "Epoch 38/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0392 - val_loss: 0.0431\n",
      "Epoch 39/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0388 - val_loss: 0.0429\n",
      "Epoch 40/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0391 - val_loss: 0.0425\n",
      "Epoch 41/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0389 - val_loss: 0.0424\n",
      "Epoch 42/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0386 - val_loss: 0.0422\n",
      "Epoch 43/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0379 - val_loss: 0.0420\n",
      "Epoch 44/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0379 - val_loss: 0.0418\n",
      "Epoch 45/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0385 - val_loss: 0.0416\n",
      "Epoch 46/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0385 - val_loss: 0.0414\n",
      "Epoch 47/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0376 - val_loss: 0.0412\n",
      "Epoch 48/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0377 - val_loss: 0.0412\n",
      "Epoch 49/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0371 - val_loss: 0.0410\n",
      "Epoch 50/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0366 - val_loss: 0.0408\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n"
     ]
    }
   ],
   "source": [
    "# Get the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(f\"Best Encoding Dim: {best_hps.get('encoding_dim')}\")\n",
    "print(f\"Best Layer 1 Units: {best_hps.get('units1')}, Activation: {best_hps.get('activation1')}\")\n",
    "print(f\"Best Layer 2 Units: {best_hps.get('units2')}, Activation: {best_hps.get('activation2')}\")\n",
    "print(f\"Best Learning Rate: {best_hps.get('learning_rate')}\")\n",
    "\n",
    "# Train Autoencoder with best hyperparameters\n",
    "best_autoencoder = tuner.hypermodel.build(best_hps)\n",
    "history = best_autoencoder.fit(X_train, X_train, epochs=50, batch_size=16, shuffle=True, validation_data=(X_test, X_test))\n",
    "\n",
    "# Extract Encoder Model\n",
    "encoder = Model(best_autoencoder.input, best_autoencoder.layers[1].output)  # Extract latent space representation\n",
    "\n",
    "# Generate latent space representation (Employee Scores)\n",
    "employee_scores = encoder.predict(scaled_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "                                        Importance\n",
      "Goals Score                               0.009905\n",
      "Trait Assessment Score                    0.006176\n",
      "Additional Accomplishment Score           0.005525\n",
      "Programming Score_JD                      0.005235\n",
      "KPI                                       0.004090\n",
      "Soft Score_with_JD                        0.002727\n",
      "Education_match_Score_with_JD             0.002298\n",
      "Technical Score_JD                        0.001985\n",
      "Years of Experience in this Company       0.001972\n",
      "Potential Assessment Score                0.001432\n",
      "Experience in Years Previous Positions    0.001304\n",
      "Competency Score                          0.001291\n",
      "Cultural Value Score                      0.001220\n",
      "Employee Satisfaction Score               0.000801\n"
     ]
    }
   ],
   "source": [
    "# X_test_pred = best_autoencoder.predict(X_test)\n",
    "\n",
    "# baseline_loss = np.mean(np.square(X_test - X_test_pred), axis=0)\n",
    "\n",
    "# # Feature importance via reconstruction loss increase\n",
    "# feature_importance = {}\n",
    "# input_dim = X_train.shape[1]\n",
    "# for i in range(input_dim):\n",
    "#     X_test_masked = X_test.copy()\n",
    "#     X_test_masked[:, i] = 0  # Mask one feature at a time\n",
    "#     X_test_pred_masked = best_autoencoder.predict(X_test_masked)\n",
    "#     masked_loss = np.mean(np.square(X_test - X_test_pred_masked), axis=0)\n",
    "#     feature_importance[f'Feature_{i+1}'] = np.mean(masked_loss - baseline_loss)\n",
    "\n",
    "# # Convert feature importance to a sorted DataFrame\n",
    "# feature_importance_df = pd.DataFrame.from_dict(feature_importance, orient='index', columns=['Importance'])\n",
    "# feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "# print(feature_importance_df)\n",
    "\n",
    "feature_names = df.columns.tolist()\n",
    "\n",
    "# Compute baseline reconstruction error\n",
    "X_test_pred = best_autoencoder.predict(X_test)\n",
    "baseline_loss = np.mean(np.square(X_test - X_test_pred), axis=0)\n",
    "\n",
    "# Feature importance via reconstruction loss increase\n",
    "feature_importance = {}\n",
    "input_dim = X_train.shape[1]\n",
    "for i in range(input_dim):\n",
    "    X_test_masked = X_test.copy()\n",
    "    X_test_masked[:, i] = 0  # Mask one feature at a time\n",
    "    X_test_pred_masked = best_autoencoder.predict(X_test_masked)\n",
    "    masked_loss = np.mean(np.square(X_test - X_test_pred_masked), axis=0)\n",
    "    feature_importance[feature_names[i]] = np.mean(masked_loss - baseline_loss)\n",
    "\n",
    "# Convert feature importance to a sorted DataFrame\n",
    "feature_importance_df = pd.DataFrame.from_dict(feature_importance, orient='index', columns=['Importance'])\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Display the results\n",
    "print(feature_importance_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add latent features to dataset\n",
    "for i in range(employee_scores.shape[1]):\n",
    "    data[f'LatentFeature_{i+1}'] = employee_scores[:, i]\n",
    "\n",
    "     \n",
    "# # Save preprocessed data with latent features\n",
    "# df.to_excel(\"employee_autoencoder_scores.xlsx\", index=False)\n",
    "\n",
    "# Save models and scaler\n",
    "# best_autoencoder.save(\"best_autoencoder_model.h5\")\n",
    "# encoder.save(\"encoder_model.h5\")\n",
    "# import joblib\n",
    "# joblib.dump(scaler, \"scaler.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install keras_tuner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['SuitabilityScore_AE'] = np.abs(data[[f'LatentFeature_{i+1}' for i in range(employee_scores.shape[1])]]).mean(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.read_excel(\"C://Users//DanukaDilshanRathnay//Desktop//AI-Driven-Job-Role-Fit-Prediction//code//Dataset//Jd_score.xlsx\")\n",
    "df3=pd.read_excel(\"C://Users//DanukaDilshanRathnay//Desktop//AI-Driven-Job-Role-Fit-Prediction//code//PCA_score.xlsx\")\n",
    "df1=df1[['EmployeeCode','JD match Score']]\n",
    "df1['Job_disimilarity']=1-df1['JD match Score']\n",
    "df3=df3[['EmployeeCode','Suitability_score_scaled_PCA']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAE = pd.concat([employee_ids.reset_index(drop=True), data], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAE=dfAE[['EmployeeCode','SuitabilityScore_AE']]\n",
    "Suitability_Scores=df1.merge(df3,on='EmployeeCode')\n",
    "Suitability_Scores=Suitability_Scores.merge(dfAE,on='EmployeeCode')\n",
    "Suitability_Scores.to_excel(\"abc.xlsx\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['EmployeeCode', 'JD match Score', 'Job_disimilarity',\n",
       "       'Suitability_score_scaled_PCA', 'SuitabilityScore_AE'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Suitability_Scores.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job_disimilarity</th>\n",
       "      <th>Suitability_score_scaled_PCA</th>\n",
       "      <th>SuitabilityScore_AE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Job_disimilarity</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.230778</td>\n",
       "      <td>0.146850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Suitability_score_scaled_PCA</th>\n",
       "      <td>-0.230778</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.266166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SuitabilityScore_AE</th>\n",
       "      <td>0.146850</td>\n",
       "      <td>0.266166</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Job_disimilarity  Suitability_score_scaled_PCA  \\\n",
       "Job_disimilarity                      1.000000                     -0.230778   \n",
       "Suitability_score_scaled_PCA         -0.230778                      1.000000   \n",
       "SuitabilityScore_AE                   0.146850                      0.266166   \n",
       "\n",
       "                              SuitabilityScore_AE  \n",
       "Job_disimilarity                         0.146850  \n",
       "Suitability_score_scaled_PCA             0.266166  \n",
       "SuitabilityScore_AE                      1.000000  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Suitability_Scores[['Job_disimilarity','Suitability_score_scaled_PCA', 'SuitabilityScore_AE']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Suitability_Scores[['Job_disimilarity','Suitability_score_scaled_PCA', 'SuitabilityScore_AE']].to_excel(\"ff.xlsx\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   Feature  Importance\n",
      "13                  Trait Assessment Score    2.237420\n",
      "5                                      KPI    2.002982\n",
      "8                              Goals Score    1.915216\n",
      "10                    Cultural Value Score    1.776136\n",
      "1                     Programming Score_JD    1.738813\n",
      "6              Employee Satisfaction Score    1.722330\n",
      "7   Experience in Years Previous Positions    1.695453\n",
      "4      Years of Experience in this Company    1.635422\n",
      "12              Potential Assessment Score    1.624480\n",
      "0                       Technical Score_JD    1.535799\n",
      "11         Additional Accomplishment Score    1.396079\n",
      "9                         Competency Score    1.304043\n",
      "2                       Soft Score_with_JD    1.218901\n",
      "3            Education_match_Score_with_JD    1.215432\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# ✅ Ensure feature names exist\n",
    "if isinstance(df, pd.DataFrame):\n",
    "    feature_names = df.columns.tolist()  # Get actual column names\n",
    "else:\n",
    "    raise ValueError(\"df must be a Pandas DataFrame to retrieve actual column names.\")\n",
    "\n",
    "# ✅ Identify the first Dense layer in the encoder\n",
    "dense_layers = [layer for layer in encoder.layers if isinstance(layer, Dense)]\n",
    "if not dense_layers:\n",
    "    raise ValueError(\"No Dense layers found in the encoder. Check your model structure.\")\n",
    "\n",
    "first_dense_layer = dense_layers[0]  # Get the first Dense layer\n",
    "\n",
    "# ✅ Extract encoder weights\n",
    "encoder_weights = first_dense_layer.get_weights()[0]  # First dense layer weights\n",
    "\n",
    "# ✅ Ensure weight dimensions match the number of features\n",
    "if encoder_weights.shape[0] != len(feature_names):\n",
    "    raise ValueError(f\"Mismatch: Encoder weights have {encoder_weights.shape[0]} inputs, \"\n",
    "                     f\"but feature names list has {len(feature_names)} features.\")\n",
    "\n",
    "# ✅ Compute feature importance as the sum of absolute weights\n",
    "feature_importance = np.sum(np.abs(encoder_weights), axis=1)\n",
    "\n",
    "# ✅ Create a DataFrame with feature names and importance values\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    \"Feature\": feature_names,\n",
    "    \"Importance\": feature_importance\n",
    "})\n",
    "\n",
    "# ✅ Sort features by importance (descending)\n",
    "feature_importance_df = feature_importance_df.sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# ✅ Display the most influential features\n",
    "print(feature_importance_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
